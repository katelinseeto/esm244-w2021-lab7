---
title: 'Part 2: Text Analysis with The Hobbit'
author: "Katelin Seeto"
date: "2/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidytext)
library(textdata)
library(pdftools)
library(ggwordcloud)
```

### Read in The Hobbit with `pdftools::pdf_text()` since text is in pdf format
* When you open that text notice that line separation is shown by \n
* Each page of text doc is stored its own element inside a huge vector containing all the text. 
* Ex) page 34 is 34th element
```{r, cache = TRUE}

hobbit_text <- pdf_text("the-hobbit.pdf")

# Since file is large and it take a long time to run function we don't want this too run every time we knit
# add cache = TRUE to code chunk header so it won't re-run that code when you're knitting unless a change has been made to the code 
```

### Search for text on page 34 
```{r}
hobbit_text_p34 <- hobbit_text[34]
hobbit_text_p34

# Notice it's still not tidy

```

### Take entire text, split it up by line, then trim excess whitespace 
* Use `str_split()` to recognize anywhere there is a line break (\n) and split that up and then un-nest them into their own. Must write out as "\\n" since \ tells R that it should recognize what's coming next as a character, don't assume it's something else
* `unnest()` to take each element of each vector and give it it's own space 
```{r}
hobbit_tidy <- data.frame(hobbit_text) %>%  #convert hobbit text into df, each row is now a page
  mutate(text_full = str_split(hobbit_text, pattern = '\\n')) %>%  # text_full contains a series of strings which is each line but is still only part of same element
  unnest(text_full) %>% # hobbit_text is chapter info but now each line of text, has it's own line in df in the text_full col
  mutate(text_full = str_trim(text_full)) #remove whitespace

```

### Wrangle so that there is grouping variable that indicates the chapter number so we can do analysis comparing differt chapters 
* use `str_detect()` to detect the word "chapter"
* Will create a new column to list chapter and will duplicate for when "Chapter" appears but will list NA if it doesn't 
* use mutate(chapter = case_when()) since it will be a conditional column
* must specify what class is associated with your NA values in this column, in this case NA_character
* then will use `fill()` to fill in that chapter number to any NA's below it until it gets to next non-NA value
```{r}
hobbit_df <- hobbit_tidy %>% 
  slice(-(1:125)) %>% # remove title pg, table contents etc
  mutate(chapter = case_when(
    str_detect(text_full, pattern = "Chapter") ~ text_full, 
    TRUE ~ NA_character_
  )) %>% 
  fill(chapter)
```

### Create new column with just chapter # so when we plot it won't be in alphabetical order 
*`separate()` to separate chapt # into it's own column
* However since chapter number is a roman numeral it's intitally recognized as a character. 
* Use mutate to create new col called chapter which is a numeric version of the roman numeral version of the number col

```{r}
 hobbit_df <- hobbit_df %>% 
  separate(col = chapter, into = c("ch", "no"), sep = " ") %>% 
  mutate(chapter = as.numeric(as.roman(no)))
```

### Get into tokenized version where one token is a single word
* tidy format for text analysis
* `unnest_tokens()` function to split columns into tokens, flattening the table into one token per row. unnest_tokens(name of new column, source you'll break up)

```{r}
hobbit_tokens <- hobbit_df %>% 
  unnest_tokens(word, text_full) %>% 
  select(-hobbit_text)

# Get word count by chapter
hobbit_wordcount <- hobbit_tokens %>% 
  count(chapter, word) 
```

### Remove all stop_words that exist in hobbit_tokens
* stop_words are words like a, the, of, etc. 
* use `anti_join()`to get rid of anything that shows up in hobbit_tokens and also in stop_words

```{r}
hobbit_nonstop_words <- hobbit_tokens %>% 
  anti_join(stop_words)

nonstop_counts <- hobbit_nonstop_words %>% 
  count(chapter, word)
```

### Find top 5 words by chapter
```{r}
top_5_words <- nonstop_counts %>% 
  group_by(chapter) %>% 
  arrange(-n) %>% 
  slice(1:5)

#plot these counts 
ggplot(data = top_5_words, aes(x = word, y = n)) + 
  geom_col(fill = "blue") + 
  facet_wrap(~chapter, scales = "free") + 
  coord_flip()
```

### Make word cloud 
```{r}
# Get top 100 words in Ch 1
ch1_top100 <- nonstop_counts %>% 
  filter(chapter == 1) %>% 
  arrange(-n) %>% 
  slice(1:100)
# Not the most effective way to do this, could do group_by chapter and then slice_max

ch1_cloud <- ggplot(data = ch1_top100, aes(label = word)) + 
  geom_text_wordcloud(aes(color = n, size = n)) + 
  scale_size_area(max_size = 6)

ch1_cloud

```
### Sentiment analysis of each chapter with `afinn` lexicon
*`get_sentiments()` and then specify which lexicon to look at 
* `inner_join()` will only keep words in the hobbit that match with words in afinn
```{r}
affin_pos <- get_sentiments("afinn") %>% 
  filter(value > 2)  # get words that are afinn score of 3,4,5
  
# With `afinn`
hobbit_afinn <- hobbit_nonstop_words %>% 
  inner_join(get_sentiments("afinn"))

affin_counts <- hobbit_afinn %>% 
  count(chapter, value)

# Mean sentiment score with `afinn`
afinn_means <- hobbit_afinn %>% 
  group_by(chapter) %>% 
  summarise(mean_afinn = mean(value))

ggplot(data = afinn_means, 
       aes(x = chapter, y = mean_afinn)) + 
  geom_col() +
  coord_flip()

```
### Sentiment analysis of each chapter with NRC lexicon
```{r}
hobbit_nrc <- hobbit_nonstop_words %>% 
  inner_join(get_sentiments("nrc"))

hobbit_nrc_counts <- hobbit_nrc %>% 
  count(chapter, sentiment)

ggplot(data = hobbit_nrc_counts, aes(x = sentiment, y = n)) +
  geom_col() +
  facet_wrap(~chapter) + 
  coord_flip()

```



